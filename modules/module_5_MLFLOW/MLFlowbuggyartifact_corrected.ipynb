{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdf58029-1857-4059-b568-b4945a0c364d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc50ae5-8460-4704-9672-c042dd4cbf2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "# Replace with your details\n",
    "storage_account_name = \"retaildatasetsblob\"\n",
    "storage_account_key = \"cRoMjWAkykST/Jn+RfOFmMKqfeQ6LEVlVbkUqRkNH/bUMAI0iUnPm7Lw8YHHhXM6Wjzr0ogqaFmo+AStAhgIcg==\"\n",
    "\n",
    "# Connect to ADLS\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n",
    "    credential=storage_account_key,\n",
    "    api_version=\"2023-11-03\"  # Use the correct supported API version\n",
    ")\n",
    "\n",
    "# List Containers\n",
    "containers = service_client.list_file_systems()\n",
    "for container in containers:\n",
    "    print(container.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8de8b20-fbd7-44cc-997c-aa1b5efa0920",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "def read_csv_from_blob(storage_account_name, container_name, file_name, storage_account_key=None):\n",
    "    \"\"\"\n",
    "    Read a CSV file from Azure Blob Storage using Python and return a Pandas DataFrame.\n",
    "\n",
    "    :param storage_account_name: Azure storage account name.\n",
    "    :param container_name: Blob container name.\n",
    "    :param file_name: Name of the file in the container.\n",
    "    :param storage_account_key: Storage account access key.\n",
    "    :return: Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not storage_account_key:\n",
    "        # Try to get the key from environment variables if not provided\n",
    "        storage_account_key = os.environ.get('AZURE_STORAGE_KEY')\n",
    "        \n",
    "    if not storage_account_key:\n",
    "        raise ValueError(\"Storage account key must be provided either as a parameter or as an environment variable 'AZURE_STORAGE_KEY'\")\n",
    "    \n",
    "    try:\n",
    "        # Create a connection string\n",
    "        connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "        \n",
    "        # Create the BlobServiceClient\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "        \n",
    "        # Get the container client\n",
    "        container_client = blob_service_client.get_container_client(container_name)\n",
    "        \n",
    "        # Get the blob client\n",
    "        blob_client = container_client.get_blob_client(file_name)\n",
    "        \n",
    "        # Download the blob content\n",
    "        download_stream = blob_client.download_blob()\n",
    "        \n",
    "        # Convert the content to a DataFrame\n",
    "        content = download_stream.readall()\n",
    "        df = pd.read_csv(io.BytesIO(content))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cdb59f9-ec1c-47ae-9dbf-caa876265e07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "training_data = read_csv_from_blob(storage_account_name=storage_account_name,\n",
    "                                      container_name=\"globalmartmarketingdata\", \n",
    "                                      file_name=\"PreProcessing_final_data.csv\",\n",
    "                                      storage_account_key=storage_account_key)\n",
    "training_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13d00d56-e71e-4d21-aa9a-2b7291d2374a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Final Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "492887fc-43c0-4525-a187-482baf3b7ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select features for scaling and encoding\n",
    "numerical_features = ['sales_amount', 'base_price', 'final_price',\n",
    "                      'facebook_spend', 'google ads_spend', 'influencer marketing_spend',\n",
    "                      'instagram_spend', 'ooh_spend', 'print_spend', 'radio_spend',\n",
    "                      'tv_spend', 'youtube_spend', 'facebook_ctr', 'google ads_ctr',\n",
    "                      'influencer marketing_ctr', 'instagram_ctr', 'youtube_ctr']\n",
    "\n",
    "categorical_features = ['promotion_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9abe5878-a592-493f-af6d-081240185a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "scaled_numerical_data = scaler.fit_transform(training_data[numerical_features])\n",
    "\n",
    "# Convert scaled numerical data to a DataFrame\n",
    "scaled_numerical_df = pd.DataFrame(scaled_numerical_data, columns=numerical_features)\n",
    "\n",
    "# Step 2: Encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_categorical_data = encoder.fit_transform(training_data[categorical_features])\n",
    "\n",
    "# Get column names for encoded categorical features\n",
    "encoded_categorical_columns = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Convert encoded categorical data to a DataFrame\n",
    "encoded_categorical_df = pd.DataFrame(encoded_categorical_data, columns=encoded_categorical_columns)\n",
    "\n",
    "# Step 3: Combine scaled numerical data and encoded categorical data\n",
    "processed_df = pd.concat([scaled_numerical_df, encoded_categorical_df], axis=1)\n",
    "\n",
    "# Display the processed DataFrame\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae4cc16b-4272-4740-b29f-1cdac1363bac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check if there are any zero or negative values in the numerical columns\n",
    "print((processed_df[numerical_features] <= 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a3cb293-8f75-4ec7-883c-db04c77cb1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace zero or negative values with a small positive value (e.g., 1e-6)\n",
    "processed_df[numerical_features] = processed_df[numerical_features].applymap(lambda x: max(x, 1e-6))\n",
    "\n",
    "# Now apply log1p to handle the log transformation safely\n",
    "X_log = np.log1p(processed_df[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"optimizing-ad-spend-experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59074b09-363f-4216-a26b-c35ddee57a4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b24b2610-6cf7-43aa-9451-3ec779af87df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"Linear Regression\"):\n",
    "    # Add constant term to X for the intercept in OLS regression\n",
    "    X_const = sm.add_constant(X)\n",
    "    \n",
    "    # Build the additive linear regression model with statsmodels\n",
    "    model = sm.OLS(y, X_const).fit()\n",
    "    \n",
    "    # Print the summary of the model \n",
    "    print(model.summary())\n",
    "    \n",
    "    # Extract the coefficients of the model\n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': X_const.columns,\n",
    "        'Coefficient': model.params\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "    \n",
    "    # Logging model metrics  \n",
    "    mlflow.log_metric(\"r_squared\", model.rsquared)  # R-squared value  \n",
    "    mlflow.log_metric(\"adjusted_r_squared\", model.rsquared_adj)  # Adjusted R-squared  \n",
    "    mlflow.log_metric(\"f_statistic\", model.fvalue)  # F-statistic  \n",
    "    mlflow.log_metric(\"f_pvalue\", model.f_pvalue)  # P-value for F-statistic  \n",
    "  \n",
    "    # Log residual metrics  \n",
    "    residuals = model.resid\n",
    "    mlflow.log_metric(\"mean_residuals\", residuals.mean())  # Mean of residuals  \n",
    "    mlflow.log_metric(\"std_residuals\", residuals.std())  # Standard deviation of residuals  \n",
    "    \n",
    "    # Save the coefficients DataFrame as a CSV and log it as an artifact  \n",
    "    coefficients_file_path = \"/tmp/coefficients_ols.csv\"  \n",
    "    coefficients.to_csv(coefficients_file_path, index=False)  # Saving DataFrame to CSV file  \n",
    "    mlflow.log_artifact(coefficients_file_path)  # Log the CSV file as an artifact  \n",
    "  \n",
    "    # Log residuals as an artifact  \n",
    "    residuals_file_path = \"/tmp/residuals_ols.csv\"  \n",
    "    residuals_df = pd.DataFrame({'Residuals': residuals})  \n",
    "    residuals_df.to_csv(residuals_file_path, index=False)  \n",
    "    mlflow.log_artifact(residuals_file_path)  # Log residuals as an artifact\n",
    "    \n",
    "    # Logging coefficients \n",
    "    for feature, coef in zip(coefficients['Feature'], coefficients['Coefficient']):\n",
    "        mlflow.log_metric(f\"coef_{feature}\", coef)  \n",
    "\n",
    "    # Save the coefficients DataFrame as a CSV and log it as an artifact\n",
    "    coefficients_file_path = \"/tmp/coefficients_ols.csv\"\n",
    "    coefficients.to_csv(coefficients_file_path, index=False)  # Saving DataFrame to CSV file\n",
    "    \n",
    "    # Log the CSV file as an artifact (will store it under the run's artifact directory)\n",
    "    mlflow.log_artifact(coefficients_file_path)  # Log the CSV file as an artifact\n",
    "    \n",
    "    # Logging the model with wrong version name\n",
    "    mlflow.log_param(\"model_type\", \"ols\")  \n",
    "    \n",
    "    conda_env = {  \n",
    "        'channels': ['defaults'],  \n",
    "        'dependencies': [  \n",
    "            'python=3.8',  \n",
    "            'statsmodels=0.13.2',  \n",
    "            'mlflow'  \n",
    "        ],  \n",
    "        'name': 'mlflow-env'  \n",
    "    }\n",
    "     # Infer the model signature  \n",
    "    predicted_values = model.fittedvalues  # Predicted values from the model  \n",
    "    signature = infer_signature(X_const, predicted_values)  \n",
    "    \n",
    "    # Log the model with a proper version name  \n",
    "    mlflow.statsmodels.log_model(  \n",
    "        sm_model=model,  \n",
    "        artifact_path=\"model\",  \n",
    "        registered_model_name=\"OLS_Regression_v1\"  ,\n",
    "        conda_env=conda_env,\n",
    "        signature=signature\n",
    "    )\n",
    "    # Feature importance bar chart  \n",
    "    plt.figure(figsize=(10, 6))  \n",
    "    sns.barplot(x=\"Coefficient\", y=\"Feature\", data=coefficients, palette=\"viridis\")  \n",
    "    plt.title(\"Feature Coefficients\")  \n",
    "    plt.xlabel(\"Coefficient Value\")  \n",
    "    plt.ylabel(\"Feature\")  \n",
    "    feature_importance_path = \"/tmp/feature_coefficients.png\"  \n",
    "    plt.savefig(feature_importance_path)  \n",
    "    mlflow.log_artifact(feature_importance_path)  # Log feature importance plot as an artifact  \n",
    "    plt.close()\n",
    "    \n",
    "    mlflow.register_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc27492f-ff7c-404b-bd0a-66c2676bc2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## log-log regression training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4be697f7-7e5d-4f67-aefa-7829acb06593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply log transformation to the target variable (sales_amount) and features (X)\n",
    "target_column = 'sales_amount'\n",
    "X = processed_df.drop(columns=[target_column])\n",
    "y = processed_df[target_column]\n",
    "\n",
    "# Apply log transformation (log-log regression)\n",
    "X_log = np.log1p(X)  # log(1 + x) to handle zero and negative values\n",
    "y_log = np.log1p(y)  # log(1 + y) to handle zero and negative values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14a4d983-3d35-4364-8a41-a20451592a20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow run \n",
    "with mlflow.start_run(run_name=\"log-log-regression\"):\n",
    "    # Initialize and train the linear regression model on the log-transformed data\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_log, y_log)\n",
    "    \n",
    "    # Extract the coefficients of the model \n",
    "    coefficients = pd.DataFrame({\n",
    "        'Feature': X_log.columns,\n",
    "        'Coefficient': model.coef_\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "    # Logging the metrics and parameters\n",
    "    mlflow.log_metric(\"r_squared\", model.score(X_log, y_log))  # Logging model score\n",
    "    mlflow.log_param(\"model_type\", \"linear\")  # Logging a param \n",
    "    \n",
    "    # Logging coefficients as metrics\n",
    "    for feature, coef in zip(coefficients['Feature'], coefficients['Coefficient']):\n",
    "        mlflow.log_metric(f\"coef_{feature}\", coef) \n",
    "\n",
    "    # Save the coefficients DataFrame as a CSV and log it as an artifact\n",
    "    coefficients_file_path = \"/tmp/coefficients.csv\"\n",
    "    coefficients.to_csv(coefficients_file_path, index=False)  # Saving DataFrame to CSV file\n",
    "    \n",
    "    # Log the CSV file as an artifact (will store it under the run's artifact directory)\n",
    "    mlflow.log_artifact(coefficients_file_path)  # Log the CSV file as an artifact\n",
    "    \n",
    "    # Logging the model \n",
    "    mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"LinearRegression_v1\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea4879e1-421a-45e4-9f0e-d38d5aa9f0b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c29710fd-5fa6-4d25-bc31-83e00eafb0d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"lasso-regression\"):\n",
    "    # Initialize Lasso model with a regularization parameter (alpha)\n",
    "    lasso_model = Lasso(alpha=0.1)  \n",
    "    \n",
    "    # Train the model on log-transformed features and target\n",
    "    lasso_model.fit(X_log, y_log)\n",
    "    \n",
    "    # Get the coefficients of the Lasso model\n",
    "    lasso_coefficients = pd.DataFrame({\n",
    "        'Feature': X_log.columns,\n",
    "        'Coefficient': lasso_model.coef_\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "    \n",
    "    # Logging the model score \n",
    "    mlflow.log_metric(\"r_squared\", lasso_model.score(X_log, y_log))\n",
    "    \n",
    "    # Logging coefficients \n",
    "    for feature, coef in zip(lasso_coefficients['Feature'], lasso_coefficients['Coefficient']):\n",
    "        mlflow.log_metric(f\"coef_{feature}\", coef)  \n",
    "    \n",
    "    # Save the coefficients DataFrame as a CSV and log it as an artifact\n",
    "    coefficients_file_path = \"/tmp/lasso_coefficients.csv\"\n",
    "    lasso_coefficients.to_csv(coefficients_file_path, index=False)  # Saving DataFrame to CSV file\n",
    "    \n",
    "    # Log the CSV file as an artifact \n",
    "    mlflow.log_artifact(coefficients_file_path)  # Log the CSV file as an artifact\n",
    "    \n",
    "    # Logging the model \n",
    "    mlflow.log_param(\"model_type\", \"lasso\") \n",
    "    mlflow.sklearn.log_model(lasso_model, \"model\", registered_model_name=\"Lasso_v1\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "723c3e6e-f01d-4fb0-8f33-0628967e4486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f5419f2-f99a-411d-af47-555967ac62c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow run \n",
    "with mlflow.start_run(run_name=\"ridge-regression\"):\n",
    "    # Initialize Ridge model with a regularization parameter (alpha)\n",
    "    ridge_model = Ridge(alpha=0.1)  \n",
    "    \n",
    "    # Train the model on log-transformed features and target\n",
    "    ridge_model.fit(X_log, y_log)\n",
    "    \n",
    "    # Get the coefficients of the Ridge model\n",
    "    ridge_coefficients = pd.DataFrame({\n",
    "        'Feature': X_log.columns,\n",
    "        'Coefficient': ridge_model.coef_\n",
    "    }).sort_values(by='Coefficient', ascending=False)\n",
    "    \n",
    "    # Logging the model score \n",
    "    mlflow.log_metric(\"r_squared\", ridge_model.score(X_log, y_log))  \n",
    "    \n",
    "    # Logging coefficients \n",
    "    for feature, coef in zip(ridge_coefficients['Feature'], ridge_coefficients['Coefficient']):\n",
    "        mlflow.log_metric(f\"coef_{feature}\", coef)  \n",
    "    \n",
    "    # Save the coefficients DataFrame as a CSV and log it as an artifact\n",
    "    coefficients_file_path = \"/tmp/ridge_coefficients.csv\"\n",
    "    ridge_coefficients.to_csv(coefficients_file_path, index=False)  # Saving DataFrame to CSV file\n",
    "    \n",
    "    # Log the CSV file as an artifact (will store it under the run's artifact directory)\n",
    "    mlflow.log_artifact(coefficients_file_path)  # Log the CSV file as an artifact\n",
    "    \n",
    "    # Logging the model \n",
    "    mlflow.log_param(\"model_type\", \"ridge\")  \n",
    "    mlflow.sklearn.log_model(ridge_model, \"model\", registered_model_name=\"Ridge_v1\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ad2319b-ed75-47b2-bc46-21e359a053bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Start MLflow run \n",
    "with mlflow.start_run(run_name=\"random-forest\"):\n",
    "    # Train the first Random Forest Regressor model\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X, y)\n",
    "    \n",
    "    # Extract feature importances from the first model\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Logging feature importances \n",
    "    for feature, importance in zip(feature_importances['Feature'], feature_importances['Importance']):\n",
    "        mlflow.log_metric(f\"importance_{feature}\", importance) \n",
    "\n",
    "    # Train the second Random Forest Regressor model\n",
    "    rf_model_1 = RandomForestRegressor(n_estimators=30, random_state=42)\n",
    "    rf_model_1.fit(X, y)\n",
    "    \n",
    "    # Extract feature importances from the second model\n",
    "    feature_importances_1 = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': rf_model_1.feature_importances_\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Logging feature importances for the second model\n",
    "    for feature, importance in zip(feature_importances_1['Feature'], feature_importances_1['Importance']):\n",
    "        mlflow.log_metric(f\"importance_{feature}_model_2\", importance)  \n",
    "    \n",
    "    # Save the feature importances DataFrame from the first model as a CSV and log it as an artifact\n",
    "    coefficients_file_path = \"/tmp/rf_model_1_importances.csv\"\n",
    "    feature_importances.to_csv(coefficients_file_path, index=False)  # Saving feature importances to CSV\n",
    "    \n",
    "    # Log the CSV file from the first model as an artifact (will store it under the run's artifact directory)\n",
    "    mlflow.log_artifact(coefficients_file_path)  # Log the CSV file as an artifact\n",
    "    \n",
    "    # Save the feature importances DataFrame from the second model as a CSV and log it as an artifact\n",
    "    coefficients_file_path_1 = \"/tmp/rf_model_2_importances.csv\"\n",
    "    feature_importances_1.to_csv(coefficients_file_path_1, index=False)  # Saving feature importances to CSV\n",
    "    \n",
    "    # Log the CSV file from the second model as an artifact (will store it under the run's artifact directory)\n",
    "    mlflow.log_artifact(coefficients_file_path_1)  # Log the CSV file as an artifact\n",
    "    \n",
    "    # Logging the models with wrong version names\n",
    "    mlflow.log_param(\"model_type\", \"random_forest\")  # Wrong model type as parameter\n",
    "    mlflow.sklearn.log_model(rf_model, \"model_1\", registered_model_name=\"RandomForest_v1\")  \n",
    "    mlflow.sklearn.log_model(rf_model_1, \"model_2\", registered_model_name=\"RandomForest_v2\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a71ff5b-f9da-4daf-b527-9b11fe635f66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# mlflow.set_experiment(\"hyperopt-experiment\") \n",
    "\n",
    "# Define the objective function for Hyperopt with some messy elements\n",
    "def objective(params):\n",
    "    # Initialize the Random Forest Regressor with hyperparameters from Hyperopt\n",
    "    rf_model = RandomForestRegressor(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        max_depth=params['max_depth'],\n",
    "        min_samples_split=params['min_samples_split'],\n",
    "        min_samples_leaf=params['min_samples_leaf'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Calculate Mean Squared Error (MSE) as the objective function to minimize\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    # Logging metrics \n",
    "    mlflow.log_metric(\"mse\", mse)  \n",
    "    mlflow.log_param(\"random_forest\", \"yes\")  \n",
    "\n",
    "    # Logging hyperparameters\n",
    "    for key, value in params.items():\n",
    "        mlflow.log_param(f\"hyperparam_{key}\", value) \n",
    "\n",
    "    # Save the model as an artifact \n",
    "    model_file_path = \"/tmp/rf_model.pkl\"\n",
    "    mlflow.sklearn.log_model(rf_model, \"model\", registered_model_name=\"RandomForest_v1\") \n",
    "\n",
    "    # Save  artifact\n",
    "    with open(model_file_path, \"w\") as f:\n",
    "        f.write(\"This is a broken model file.\")  \n",
    "    mlflow.log_artifact(model_file_path)  \n",
    "\n",
    "    return mse  \n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    'n_estimators': hp.choice('n_estimators', [50, 100, 150, 200]),  # Number of trees\n",
    "    'max_depth': hp.choice('max_depth', [None, 10, 20, 30]),  # Max depth of trees\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10]),  # Min samples required to split\n",
    "    'min_samples_leaf': hp.choice('min_samples_leaf', [1, 2, 4]),  # Min samples required in a leaf\n",
    "}\n",
    "\n",
    "# Initialize a Trials object to track the optimization process\n",
    "trials = Trials()\n",
    "\n",
    "# Start the MLflow experiment run\n",
    "with mlflow.start_run(run_name=\"hyperopt-randomforest\"):\n",
    "    # Run the optimization\n",
    "    best = fmin(fn=objective,  # The objective function to minimize\n",
    "                space=search_space,  # The search space\n",
    "                algo=tpe.suggest,  # The optimization algorithm (TPE)\n",
    "                max_evals=50,  # Number of evaluations to perform\n",
    "                trials=trials)  # Store the results in Trials object\n",
    "\n",
    "    # Print the best hyperparameters found by Hyperopt\n",
    "    print(\"Best Hyperparameters:\", best)\n",
    "\n",
    "    # Log the trials in a messy way\n",
    "    for trial in trials:\n",
    "        mlflow.log_param(\"trial_result\", str(trial)) "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MLFlow_buggy_artifact",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

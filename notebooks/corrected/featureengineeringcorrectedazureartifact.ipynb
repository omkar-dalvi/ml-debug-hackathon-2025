{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "_GFZdsKl_Axs",
      "metadata": {
        "id": "_GFZdsKl_Axs"
      },
      "source": [
        "## Data Loading\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "id": "XFXDM3ATIEIm2uhMuWkR780Y",
      "metadata": {
        "id": "XFXDM3ATIEIm2uhMuWkR780Y",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data\n"
          ]
        }
      ],
      "source": [
        "from azure.storage.filedatalake import DataLakeServiceClient\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('notebooks/corrected/.env')\n",
        "\n",
        "# Replace with your details\n",
        "storage_account_name = \"mldebugdevadls\"\n",
        "storage_account_key = os.getenv('AZURE_STORAGE_KEY')\n",
        "\n",
        "# Connect to ADLS\n",
        "service_client = DataLakeServiceClient(\n",
        "    account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n",
        "    credential=storage_account_key,\n",
        "    api_version=\"2023-11-03\"  # Use the correct supported API version\n",
        ")\n",
        "\n",
        "# List Containers\n",
        "containers = service_client.list_file_systems()\n",
        "for container in containers:\n",
        "    print(container.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "id": "7ARZLL___R0y",
      "metadata": {
        "id": "7ARZLL___R0y"
      },
      "outputs": [],
      "source": [
        "# Replace with your details\n",
        "container_name = 'data'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "id": "_GdSNyB-_VYu",
      "metadata": {
        "id": "_GdSNyB-_VYu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "def read_csv_from_blob(storage_account_name, container_name, file_name, storage_account_key=None):\n",
        "    \"\"\"\n",
        "    Read a CSV file from Azure Blob Storage using Python and return a Pandas DataFrame.\n",
        "\n",
        "    :param storage_account_name: Azure storage account name.\n",
        "    :param container_name: Blob container name.\n",
        "    :param file_name: Name of the file in the container.\n",
        "    :param storage_account_key: Storage account access key.\n",
        "    :return: Pandas DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    if not storage_account_key:\n",
        "        # Try to get the key from environment variables if not provided\n",
        "        storage_account_key = os.environ.get('AZURE_STORAGE_KEY')\n",
        "\n",
        "    if not storage_account_key:\n",
        "        raise ValueError(\"Storage account key must be provided either as a parameter or as an environment variable 'AZURE_STORAGE_KEY'\")\n",
        "\n",
        "    try:\n",
        "        # Create a connection string\n",
        "        connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
        "\n",
        "        # Create the BlobServiceClient\n",
        "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "\n",
        "        # Get the container client\n",
        "        container_client = blob_service_client.get_container_client(container_name)\n",
        "\n",
        "        # Get the blob client\n",
        "        blob_client = container_client.get_blob_client(file_name)\n",
        "\n",
        "        # Download the blob content\n",
        "        download_stream = blob_client.download_blob()\n",
        "\n",
        "        # Convert the content to a DataFrame\n",
        "        content = download_stream.readall()\n",
        "        df = pd.read_csv(io.BytesIO(content))\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV file: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "id": "jFiQINfv_Z1E",
      "metadata": {
        "id": "jFiQINfv_Z1E"
      },
      "outputs": [],
      "source": [
        "online_marketing = read_csv_from_blob(storage_account_name=storage_account_name,\n",
        "                                      container_name=container_name, \n",
        "                                      file_name=\"OnlineMarketingData.csv\",\n",
        "                                      storage_account_key=storage_account_key)\n",
        "offline_marketing = read_csv_from_blob(storage_account_name=storage_account_name,\n",
        "                                      container_name=container_name, \n",
        "                                      file_name=\"OfflineMarketingData.csv\",\n",
        "                                      storage_account_key=storage_account_key)\n",
        "sales = read_csv_from_blob(storage_account_name=storage_account_name,\n",
        "                                      container_name=container_name, \n",
        "                                      file_name=\"SalesData.csv\",\n",
        "                                      storage_account_key=storage_account_key)\n",
        "price = read_csv_from_blob(storage_account_name=storage_account_name,\n",
        "                                      container_name=container_name, \n",
        "                                      file_name=\"PricingData.csv\",\n",
        "                                      storage_account_key=storage_account_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uN20Tdx7_b2o",
      "metadata": {
        "id": "uN20Tdx7_b2o"
      },
      "source": [
        "## Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MjCZTwQb_eXx",
      "metadata": {
        "id": "MjCZTwQb_eXx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oR0pNutuBXSn",
      "metadata": {
        "id": "oR0pNutuBXSn"
      },
      "source": [
        "## Data Pre Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aPQ3DDZp_jV_",
      "metadata": {
        "id": "aPQ3DDZp_jV_"
      },
      "outputs": [],
      "source": [
        "# Read sales data and convert to date to datetime\n",
        "sales['date'] =  pd.to_datetime(sales['date'])\n",
        "\n",
        "#read offline marketing data and convert week to datetime\n",
        "offline_marketing['week'] = pd.to_datetime(offline_marketing['week'])\n",
        "\n",
        "#read online marketing data and convert to datetime\n",
        "online_marketing['date'] = pd.to_datetime(online_marketing['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pOVLSh5WAuC5",
      "metadata": {
        "id": "pOVLSh5WAuC5"
      },
      "outputs": [],
      "source": [
        "# Calculate total daily sales to match with marketing data\n",
        "daily_sales = sales.groupby('date')['sales_quantity'].sum().reset_index()\n",
        "daily_sales.rename(columns={'sales_quantity': 'total_sales'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oE8-XSupA4do",
      "metadata": {
        "id": "oE8-XSupA4do"
      },
      "outputs": [],
      "source": [
        "# Merge digital data with daily sales for analysis\n",
        "digital_sales = pd.merge(online_marketing, daily_sales, on='date', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rBE5AmXaA9u5",
      "metadata": {
        "id": "rBE5AmXaA9u5"
      },
      "outputs": [],
      "source": [
        "# Aggregate weekly sales for comparison with weekly media spend\n",
        "weekly_sales = daily_sales.copy()\n",
        "weekly_sales['week'] = pd.to_datetime(weekly_sales['date']).dt.to_period('W').dt.start_time\n",
        "weekly_sales = weekly_sales.groupby('week')['total_sales'].sum().reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9M9DS6wWBSWQ",
      "metadata": {
        "id": "9M9DS6wWBSWQ"
      },
      "outputs": [],
      "source": [
        "# Merge weekly media spend with weekly sales\n",
        "media_sales = pd.merge(offline_marketing, weekly_sales, on='week', how='left')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o7WR1ttICWAS",
      "metadata": {
        "id": "o7WR1ttICWAS"
      },
      "source": [
        "## Task 1: Flawed Adstock Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VZckFpaaBgpy",
      "metadata": {
        "id": "VZckFpaaBgpy"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# BUGGY PLOT 1: Flawed adstock transformation\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "def plot_buggy_adstock():\n",
        "    # Filter the media data to get only TV spend\n",
        "    media_df = offline_marketing[offline_marketing['channel'] == 'TV'][['week', 'spend']].copy()\n",
        "\n",
        "    # Rename 'spend' column to 'TV' for consistency\n",
        "    media_df['TV'] = media_df['spend']\n",
        "\n",
        "    # Create flawed adstock by simply lagging data\n",
        "    media_df['TV_lag1'] = media_df['TV'].shift(1)\n",
        "    media_df['TV_lag2'] = media_df['TV'].shift(2)\n",
        "\n",
        "    # Incorrectly add lags without decay\n",
        "    media_df['TV_adstock'] = media_df['TV'] + media_df['TV_lag1'] + media_df['TV_lag2']\n",
        "\n",
        "    # Plot the original and buggy adstock values\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(media_df['week'], media_df['TV'], label='Original TV Spend')\n",
        "    plt.plot(media_df['week'], media_df['TV_adstock'], label='Buggy Adstock (Simple Sum of Lags)')\n",
        "    plt.title('Buggy Adstock Transformation')\n",
        "    plt.xlabel('Week')\n",
        "    plt.ylabel('Spend')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_buggy_adstock()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QOt52FhmCfY3",
      "metadata": {
        "id": "QOt52FhmCfY3"
      },
      "source": [
        "## Task 2: Misleading Diminishing Returns Visualization (No Saturation Effect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "l3tk5FnUCiqX",
      "metadata": {
        "id": "l3tk5FnUCiqX"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# BUGGY PLOT 2: Misleading diminishing returns visualization (No saturation effect)\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "def plot_buggy_diminishing_returns():\n",
        "    # Filter the media data to get only TV spend\n",
        "    tv_spend = offline_marketing[offline_marketing['channel'] == 'TV']['spend'].values\n",
        "\n",
        "    # Create linear response (which is wrong for advertising)\n",
        "    response = 500 + 0.05 * tv_spend\n",
        "\n",
        "    # Plot the scatter plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(tv_spend, response, label='Linear Response')\n",
        "\n",
        "    # Add linear trend line\n",
        "    z = np.polyfit(tv_spend, response, 1)\n",
        "    p = np.poly1d(z)\n",
        "    plt.plot(tv_spend, p(tv_spend), \"r--\", label='Linear Trend Line')\n",
        "\n",
        "    # Add titles and labels\n",
        "    plt.title('Buggy Linear Response Curve (No Diminishing Returns)')\n",
        "    plt.xlabel('TV Spend')\n",
        "    plt.ylabel('Sales Response')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_buggy_diminishing_returns()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_UuCKfrICq5I",
      "metadata": {
        "id": "_UuCKfrICq5I"
      },
      "source": [
        "## Task 3: Incorrect Assessment of Interaction Term in Sales Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HVuc1Vl5_sQp",
      "metadata": {
        "id": "HVuc1Vl5_sQp"
      },
      "outputs": [],
      "source": [
        "offline_channels = ['TV', 'Radio', 'Print', 'OOH']\n",
        "online_channels = ['Facebook', 'Instagram', 'YouTube', 'Google Ads', 'Influencer Marketing']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QTfc1RdzDHLs",
      "metadata": {
        "id": "QTfc1RdzDHLs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def buggy_interaction_analysis():\n",
        "    \"\"\"\n",
        "    Perform Buggy Channel Interaction Analysis by summing all channel spends to create a combined spend term.\n",
        "\n",
        "    Parameters:\n",
        "    media_sales (pd.DataFrame): DataFrame containing the merged sales and marketing data.\n",
        "    offline_channels (list): List of offline marketing channels.\n",
        "    online_channels (list): List of online marketing channels.\n",
        "    \"\"\"\n",
        "    # Pivot the media_sales data to separate each channel into its own column\n",
        "    media_sales_pivoted = media_sales.pivot_table(index='week', columns='channel', values='spend', aggfunc='sum')\n",
        "\n",
        "    # Ensure that all offline and online channels are present in the DataFrame, fill missing ones with zeros\n",
        "    for channel in offline_channels + online_channels:\n",
        "        if channel not in media_sales_pivoted.columns:\n",
        "            media_sales_pivoted[channel] = 0  # Fill missing channels with zero spend\n",
        "\n",
        "    # **Buggy**: Summing all channel spends to create a combined spend (incorrect approach)\n",
        "    media_sales_pivoted['combined_spend'] = media_sales_pivoted[offline_channels + online_channels].sum(axis=1)\n",
        "\n",
        "    # **Buggy**: Interaction sales based on the combined spend (incorrect approach)\n",
        "    media_sales_pivoted['interaction_sales'] = media_sales_pivoted['combined_spend'] * 0.1  # Arbitrary multiplication without proper logic\n",
        "\n",
        "    # Merge total sales into the pivoted data\n",
        "    media_sales_pivoted = pd.merge(media_sales_pivoted, media_sales[['week', 'total_sales']], on='week', how='left')\n",
        "\n",
        "    # Plotting the actual vs misleading interaction sales (incorrect approach)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(media_sales_pivoted['week'], media_sales_pivoted['total_sales'], label='Actual Sales', color='b')\n",
        "    plt.plot(media_sales_pivoted['week'], media_sales_pivoted['interaction_sales'], label='Misleading Interaction Sales', color='r', linestyle='--')\n",
        "\n",
        "    plt.title('Buggy Channel Interaction Analysis')\n",
        "    plt.xlabel('Week')\n",
        "    plt.ylabel('Sales')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "buggy_interaction_analysis()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iLzk9sr-DVi5",
      "metadata": {
        "id": "iLzk9sr-DVi5"
      },
      "source": [
        "## Task 4: Modeling and Visualizing the Adstock Effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xb4FFIYSDU7G",
      "metadata": {
        "id": "xb4FFIYSDU7G"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# BUGGY PLOT 4: Misleading adstock effect wih wrong decay rate\n",
        "# ----------------------------\n",
        "\n",
        "def plot_adstock_effect():\n",
        "    \"\"\"\n",
        "    Function to calculate and plot the adstock effect on media spend.\n",
        "\n",
        "    Parameters:\n",
        "    - sales_data: DataFrame containing the sales data with 'date' and 'sales_quantity' columns.\n",
        "    - media_data: DataFrame containing the media spend data with 'week' and 'spend' columns.\n",
        "    - decay_rate: The decay rate to apply to the lagged spend (default is 0.5).\n",
        "\n",
        "    Returns:\n",
        "    - A plot showing the original and adstocked media spend over time.\n",
        "    \"\"\"\n",
        "\n",
        "    decay_rate=0.5\n",
        "\n",
        "    # Calculate total daily sales to match with marketing data\n",
        "    daily_sales = sales.groupby('date')['sales_quantity'].sum().reset_index()\n",
        "    daily_sales.rename(columns={'sales_quantity': 'total_sales'}, inplace=True)\n",
        "\n",
        "    # Aggregate weekly sales for comparison with weekly media spend\n",
        "    weekly_sales = daily_sales.copy()\n",
        "    weekly_sales['week'] = pd.to_datetime(weekly_sales['date']).dt.to_period('W').dt.start_time\n",
        "    weekly_sales = weekly_sales.groupby('week')['total_sales'].sum().reset_index()\n",
        "\n",
        "    # Merge media spend with weekly sales\n",
        "    media_sales = pd.merge(offline_marketing, weekly_sales, on='week', how='left')\n",
        "\n",
        "    # Apply decay rate to model adstock (using the specified decay rate)\n",
        "    media_sales['TV'] = media_sales['spend']  # Original spend\n",
        "\n",
        "    # Lag the data by 1 and 2 weeks to represent delayed effects\n",
        "    media_sales['TV_lag1'] = media_sales['spend'].shift(1) * decay_rate  # Applying decay to lag 1\n",
        "    media_sales['TV_lag2'] = media_sales['spend'].shift(2) * (decay_rate ** 2)  # Applying decay to lag 2\n",
        "\n",
        "    # Calculate the adstock effect by summing the original spend with the decayed lags\n",
        "    media_sales['TV_adstock'] = media_sales['TV'] + media_sales['TV_lag1'] + media_sales['TV_lag2']\n",
        "\n",
        "    # Plotting the results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(media_sales['week'], media_sales['TV'], label='Original TV Spend', marker='o')\n",
        "    plt.plot(media_sales['week'], media_sales['TV_adstock'], label='Adstock with Decay', marker='x', linestyle='--')\n",
        "\n",
        "    plt.title(f'Adstock Effect with Decay Rate ({decay_rate})')\n",
        "    plt.xlabel('Week')\n",
        "    plt.ylabel('Spend')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_adstock_effect()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "feature_engineering_buggy_azure_artifact",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "mldebug",
      "language": "python",
      "name": "mldebug"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

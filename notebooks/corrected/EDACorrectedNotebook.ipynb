{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa684670-f6a5-4a12-8439-88db4fee1e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63235592-8cf6-4a1e-a50a-23df71615597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv('notebooks/corrected/.env')\n",
    "\n",
    "# Replace with your details\n",
    "storage_account_name = \"mldebugdevadls\"\n",
    "storage_account_key = os.getenv('AZURE_STORAGE_KEY')\n",
    "\n",
    "# Connect to ADLS\n",
    "service_client = DataLakeServiceClient(\n",
    "    account_url=f\"https://{storage_account_name}.dfs.core.windows.net\",\n",
    "    credential=storage_account_key,\n",
    "    api_version=\"2023-11-03\"  # Use the correct supported API version\n",
    ")\n",
    "\n",
    "# List Containers\n",
    "containers = service_client.list_file_systems()\n",
    "for container in containers:\n",
    "    print(container.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7df8642f-f4b2-4d24-a2a9-aed933f6d2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Replace with your details\n",
    "container_name = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d121463e-cb21-49d3-8324-db8271e77c19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "def read_csv_from_blob(storage_account_name, container_name, file_name, storage_account_key=None):\n",
    "    \"\"\"\n",
    "    Read a CSV file from Azure Blob Storage using Python and return a Pandas DataFrame.\n",
    "\n",
    "    :param storage_account_name: Azure storage account name.\n",
    "    :param container_name: Blob container name.\n",
    "    :param file_name: Name of the file in the container.\n",
    "    :param storage_account_key: Storage account access key.\n",
    "    :return: Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not storage_account_key:\n",
    "        # Try to get the key from environment variables if not provided\n",
    "        storage_account_key = os.environ.get('AZURE_STORAGE_KEY')\n",
    "        \n",
    "    if not storage_account_key:\n",
    "        raise ValueError(\"Storage account key must be provided either as a parameter or as an environment variable 'AZURE_STORAGE_KEY'\")\n",
    "    \n",
    "    try:\n",
    "        # Create a connection string\n",
    "        connection_string = f\"DefaultEndpointsProtocol=https;AccountName={storage_account_name};AccountKey={storage_account_key};EndpointSuffix=core.windows.net\"\n",
    "        \n",
    "        # Create the BlobServiceClient\n",
    "        blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "        \n",
    "        # Get the container client\n",
    "        container_client = blob_service_client.get_container_client(container_name)\n",
    "        \n",
    "        # Get the blob client\n",
    "        blob_client = container_client.get_blob_client(file_name)\n",
    "        \n",
    "        # Download the blob content\n",
    "        download_stream = blob_client.download_blob()\n",
    "        \n",
    "        # Convert the content to a DataFrame\n",
    "        content = download_stream.readall()\n",
    "        df = pd.read_csv(io.BytesIO(content))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10dda1e0-add3-49ed-bd95-c08650894d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "online_marketing = read_csv_from_blob(storage_account_name=storage_account_name,\n",
    "                                      container_name=container_name, \n",
    "                                      file_name=\"Online_Marketing_Data.csv\",\n",
    "                                      storage_account_key=storage_account_key)\n",
    "offline_marketing = read_csv_from_blob(storage_account_name=storage_account_name,\n",
    "                                      container_name=container_name, \n",
    "                                      file_name=\"Offline_Marketing_Data.csv\",\n",
    "                                      storage_account_key=storage_account_key)\n",
    "sales = read_csv_from_blob(storage_account_name=storage_account_name,\n",
    "                                      container_name=container_name, \n",
    "                                      file_name=\"Sales_Data.csv\",\n",
    "                                      storage_account_key=storage_account_key)\n",
    "price = read_csv_from_blob(storage_account_name=storage_account_name,\n",
    "                                      container_name=container_name, \n",
    "                                      file_name=\"Pricing_Data.csv\",\n",
    "                                      storage_account_key=storage_account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d26e5c28-fb5b-4e76-a8ab-a280b3627817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ae3c72b-7427-4ff8-9226-7147ad35969f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "WWh4fKRCevnm0gM5uYvWdfZN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bcd8625a-302a-4fd0-963b-5d308b5b196b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "JwGc6nk5Nu6P"
   },
   "outputs": [],
   "source": [
    "# Read sales data and convert to date to datetime\n",
    "sales_data['date'] =  pd.to_datetime(sales_data['date'])\n",
    "\n",
    "#read offline marketing data and convert week to datetime\n",
    "offline_data['week'] = pd.to_datetime(offline_data['week'])\n",
    "\n",
    "#read online marketing data and convert to datetime\n",
    "digital_data['date'] = pd.to_datetime(digital_data['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd76756-43b8-42f1-a8b1-8bedd49cdb9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 1: Time Series Visualization with Misaligned Time Frequencies and Scale Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de61968d-2869-4265-b548-71ac570163ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "sook7YPCNztq"
   },
   "outputs": [],
   "source": [
    "# Calculate total daily sales to match with marketing data\n",
    "daily_sales = sales_data.groupby('date')['sales_quantity'].sum().reset_index()\n",
    "daily_sales.rename(columns={'sales_quantity': 'total_sales'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "212208e0-7547-4b21-bb74-abd646549f52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AknszKslN4h0"
   },
   "outputs": [],
   "source": [
    "# Merge digital data with daily sales for analysis\n",
    "digital_sales = pd.merge(digital_data, daily_sales, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce6a0470-071e-41a9-9372-4d5c9cf14511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "xAd-wS80N8I5"
   },
   "outputs": [],
   "source": [
    "# Aggregate weekly sales for comparison with weekly media spend\n",
    "weekly_sales = daily_sales.copy()\n",
    "weekly_sales['week'] = pd.to_datetime(weekly_sales['date']).dt.to_period('W').dt.start_time\n",
    "weekly_sales = weekly_sales.groupby('week')['total_sales'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef22c532-ad95-41d2-ad35-49cf62587aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "CE5NPisQOBEE"
   },
   "outputs": [],
   "source": [
    "# Merge weekly media spend with weekly sales\n",
    "media_sales = pd.merge(offline_data, weekly_sales, on='week', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0ef8c24-4dcd-43c0-97e5-dfcdcdcdc19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "oapvHRwFOdto",
    "outputId": "2d3bad43-71e6-4933-fc97-8359c3bc198f"
   },
   "outputs": [],
   "source": [
    "def plot_buggy_time_series():\n",
    "    # Pivot the media data to have channels as columns\n",
    "    offline_data_pivoted = offline_data.pivot_table(index='week', columns='channel', values='spend', aggfunc='sum')\n",
    "\n",
    "    # Plot the media spend and sales over time\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(offline_data_pivoted.index, offline_data_pivoted['TV'], label='TV Spend')\n",
    "    plt.plot(offline_data_pivoted.index, offline_data_pivoted['Radio'], label='Radio Spend')\n",
    "    plt.plot(offline_data_pivoted.index, offline_data_pivoted['Print'], label='Print Spend')\n",
    "    plt.plot(offline_data_pivoted.index, offline_data_pivoted['OOH'], label='OOH Spend')\n",
    "\n",
    "    # Sales data with arbitrary scaling\n",
    "    plt.plot(weekly_sales['week'], weekly_sales['total_sales'] * 10, label='Sales (x10)', linestyle='--')\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title('Media Spend and Sales Over Time')\n",
    "    plt.xlabel('Week')\n",
    "    plt.ylabel('Amount')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_buggy_time_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61b1f09e-e8b6-4a96-bf49-e9e977e4c609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 2: Incorrect Correlation Analysis with Mixed Frequency Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfb2e0a3-6982-4c5b-a2c4-b816815f5f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "F2o44FQxPm2g"
   },
   "outputs": [],
   "source": [
    "# Pivot the media data to have channels as columns\n",
    "offline_data_pivoted = offline_data.pivot_table(index='week', columns='channel', values='spend', aggfunc='sum')\n",
    "\n",
    "# Align with the digital data (ensure we match by the correct index)\n",
    "# Assuming digital_data has 'date' and 'clicks', we can group by date and align with weeks if necessary\n",
    "clicks_data = digital_data.groupby('date')['clicks'].sum().head(6).values  # Ensure it’s aligned by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aabf1a28-891d-4f6b-b91a-a3d25b32abc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "id": "4KpPbw3PPyLt",
    "outputId": "33e6143d-8d4f-4709-c542-30cd4f1e0eab"
   },
   "outputs": [],
   "source": [
    "def plot_buggy_correlation():\n",
    "    # Convert the media data to daily frequency by assuming spend is constant within the week\n",
    "    media_spend_daily = offline_data.copy()\n",
    "\n",
    "    # We assume each week's spend is repeated for each day of the week\n",
    "    media_spend_daily = media_spend_daily.loc[media_spend_daily['channel'] == 'TV', ['week', 'spend']]\n",
    "    media_spend_daily['date'] = pd.to_datetime(media_spend_daily['week'])\n",
    "    media_spend_daily = media_spend_daily.set_index('date').resample('D').ffill()  # Resample to daily with forward fill\n",
    "\n",
    "    # Merge datasets (now aligned by daily date)\n",
    "    merged_data = pd.merge(media_spend_daily[['spend']], digital_data[['date', 'channel', 'clicks']],\n",
    "                           left_index=True, right_on='date', how='inner')\n",
    "\n",
    "    # Now create the correct correlation DataFrame for the merged data\n",
    "    corr_data = merged_data.pivot_table(index='date', columns='channel', values='spend', aggfunc='sum')\n",
    "    corr_data['Clicks'] = merged_data.groupby('date')['clicks'].sum()\n",
    "\n",
    "    # Plot the correlation heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(corr_data.corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Corrected Correlation Heatmap (Aligned Data)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # Call the function to demonstrate the corrected plot\n",
    "plot_buggy_correlation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbfc01b4-b2e6-412e-b839-da33ae542675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 3: Misleading Seasonality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abd8077c-936b-4023-a77c-9a1266dca786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "rJVjRUAmXUoK"
   },
   "outputs": [],
   "source": [
    "def plot_buggy_seasonality():\n",
    "    # Resample sales to weekly for analysis - but choose wrong frequency\n",
    "    weekly_sales = daily_sales.copy()\n",
    "    weekly_sales.set_index('date', inplace=True)\n",
    "    weekly_sales = weekly_sales.resample('W').sum().reset_index()  # Using week ending Sunday\n",
    "\n",
    "    # Attempt decomposition without proper time series formatting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(weekly_sales['date'], weekly_sales['total_sales'])\n",
    "\n",
    "    # Add arbitrary seasonal lines without statistical basis\n",
    "    seasonal_pattern = np.sin(np.linspace(0, 4*np.pi, len(weekly_sales))) * 50 + 150\n",
    "    plt.plot(weekly_sales['date'], seasonal_pattern, 'r--', label='Assumed Seasonality')\n",
    "\n",
    "    plt.title('Buggy Seasonality Analysis')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Sales')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72233eeb-7ac2-41ec-97fa-0b2e67c828b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Task 4: Incorrect Assessment of Advertising Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d182ec0-d9cd-4df8-aed9-b5d2b5b1dc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "AvawvVbWX2r1"
   },
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# BUGGY PLOT 4: Incorrect assessment of advertising efficiency\n",
    "# ----------------------------\n",
    "\n",
    "def plot_buggy_efficiency():\n",
    "    # Calculate Cost Per Click without considering channel differences\n",
    "    digital_data['cpc'] = digital_data['spend'] / digital_data['clicks']\n",
    "\n",
    "    # Simple bar chart without context\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='channel', y='cpc', data=digital_data)\n",
    "    plt.title('Cost Per Click by Channel')\n",
    "    plt.xlabel('Channel')\n",
    "    plt.ylabel('CPC ($)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "EDA_Buggy_Notebook",
   "widgets": {}
  },
  "colab": {
   "name": "mansi_6132ffc3-f406-475e-9a90-65f50b14b4a8 (Apr 29, 2025, 4:28:40 PM)",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_debug_hackathon",
   "language": "python",
   "name": "ml_debug_hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
